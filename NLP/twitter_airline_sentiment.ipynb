{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## NATURAL LANGUAUGE PROCESSING "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Description of Data: \r\n",
    "A sentiment analysis job about the problems of each major U.S. airline. Twitter data was scraped from February of 2015 and contributors were asked to first classify positive, negative, and neutral tweets, followed by categorizing negative reasons (such as \"late flight\" or \"rude service\").\r\n",
    "\r\n",
    "\r\n",
    "#### 1. Download the file and set it as a Dataframe.\r\n",
    "Ans: For performing this step, w eneed to install spacy with en_core_web_sm, to install en_core_web_sm, use the following command in command prompt: python -m spacy download en_core_web_sm"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#importing the required packages\r\n",
    "import pandas as pd\r\n",
    "import spacy\r\n",
    "import nlp\r\n",
    "import nltk\r\n",
    "from nltk.corpus import stopwords\r\n",
    "from spacy import displacy\r\n",
    "import re\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "from sklearn.feature_extraction.text import CountVectorizer\r\n",
    "from sklearn.naive_bayes import MultinomialNB\r\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\r\n",
    "from sklearn import metrics\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "\r\n",
    "nltk.download(\"stopwords\") \r\n",
    "nltk.download('punkt')\r\n",
    "nltk.download('averaged_perceptron_tagger')\r\n",
    "nlp = spacy.load('en_core_web_sm')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 2.  Remove punctuations, special characters and stopwords from the text column. Convert the text to lower case."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#Read Source file and set it as a Dataframe\r\n",
    "df_read = pd.read_csv('Tweets.csv',dtype=object)\r\n",
    "df_read['text'] .fillna('', inplace=True)\r\n",
    "\r\n",
    "#Remove punctuations, special characters\r\n",
    "df_read[\"mod_text\"] = [re.sub(\"[^A-Za-z0-9]\",\" \",text) for text in df_read[\"text\"]]\r\n",
    "\r\n",
    "#Remove stop words & lowercase\r\n",
    "english_stopwords = stopwords.words('english')\r\n",
    "for index, row in df_read.iterrows():\r\n",
    "    wordlist = []\r\n",
    "    for wordsplit in row['mod_text'].split(\" \"):\r\n",
    "        if wordsplit.lower().strip()!='' and wordsplit.lower() not in english_stopwords:\r\n",
    "            wordlist.append(wordsplit)\r\n",
    "    df_read.at[index,'clean_data'] = ' '.join(wordlist).lower()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 3.  Create two objects X and y. X will be the 'text' column of dataframe and y will be the 'airline_sentiment’ column. create a CountVectorizer object and split the data into training and testing sets. Train a MultinomialNB model and Display the confusion Matrix"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "X = df_read.clean_data\r\n",
    "y = df_read.airline_sentiment\r\n",
    "\r\n",
    "# split X and y into training and testing sets \r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)\r\n",
    "\r\n",
    "# Create vectorizer\r\n",
    "vector = CountVectorizer()\r\n",
    "\r\n",
    "# Get the training vectors\r\n",
    "X_train_vec = vector.fit_transform(X_train)\r\n",
    "\r\n",
    "# Transforming testing data (using fitted vocabulary) into a document-term matrix\r\n",
    "X_test_vec = vector.transform(X_test)\r\n",
    "\r\n",
    "#  Multinomial Naive Bayes model instantiation\r\n",
    "nb = MultinomialNB()\r\n",
    "nb.fit(X_train_vec,y_train)\r\n",
    "\r\n",
    "# Build the classifier# make class predictions for X_test_vect\r\n",
    "y_pred_class = nb.predict(X_test_vec)\r\n",
    "\r\n",
    "# Accuracy of class predictions\r\n",
    "print(metrics.accuracy_score(y_test,y_pred_class))\r\n",
    "\r\n",
    "# calculate confusion matrix\r\n",
    "cm = confusion_matrix(y_test,y_pred_class)\r\n",
    "\r\n",
    "# print confusion matrix\r\n",
    "print(cm)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "cm_matrix = pd.DataFrame(cm, index = ['negative', 'neutral', 'positive'], columns = ['negative', 'neutral', 'positive'])\r\n",
    "\r\n",
    "#Plotting the confusion matrix\r\n",
    "plt.figure(figsize=(10,7))\r\n",
    "sns.heatmap(cm_matrix, annot=True, fmt='d')\r\n",
    "plt.title('Confusion Matrix')\r\n",
    "plt.ylabel('Actual Values')\r\n",
    "plt.xlabel('Predicted Values')\r\n",
    "plt.show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 4.  Display the POS tagging on the first 4 rows of ‘text’"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def display_posTag(col_name):\r\n",
    "    for index, row in df_read.iloc[:4].iterrows():\r\n",
    "        indx_num = df_read.columns.get_loc(col_name)\r\n",
    "        data = row[indx_num]\r\n",
    "        print('############ the POS tagging for the row: ############')\r\n",
    "        word_token =nltk.word_tokenize(str(data))\r\n",
    "        print(nltk.pos_tag(word_token))\r\n",
    "        print(\"#################################################################################################\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#pos tagging for original tweet - df column : text\r\n",
    "display_posTag('text')\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "##pos tagging for clean tweet - df column : clean_data\r\n",
    "display_posTag('clean_data') "
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### 5. Build and display a dependency parser tree for the sentence: “Ned's eldest son, Robb, gathered an army and rose up in rebellion against the Lannisters, trying to win the independence of his kingdom.”"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "\r\n",
    "doc = nlp(\"Ned's eldest son, Robb, gathered an army and rose up in rebellion against the Lannisters, trying to win the independence of his kingdom.\")\r\n",
    "print(displacy.render((doc),jupyter=True))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('tf2_gpu': conda)"
  },
  "interpreter": {
   "hash": "b53974051e1a950c2cc4d754aa4a76bed398d53900ac3964926d539f65c8ad3f"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}